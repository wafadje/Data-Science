{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02-Fashion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://images.unsplash.com/photo-1512436991641-6745cdb1723f?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1050&q=80)\n",
    "\n",
    "Photo by [Lauren Fleischmann](https://unsplash.com/photos/R2aodqJn3b8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will try to use a neural network on a simple classification task: classifying images on clothes into 10 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first download the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wafa/bin/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wafa/bin/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains 10 classes:\n",
    "* 0:\tT-shirt/top\n",
    "* 1:\tTrouser\n",
    "* 2:\tPullover\n",
    "* 3:\tDress\n",
    "* 4:\tCoat\n",
    "* 5:\tSandal\n",
    "* 6:\tShirt\n",
    "* 7:\tSneaker\n",
    "* 8:\tBag\n",
    "* 9:\tAnkle boot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now begin by exploring the data. Try to display some images with the associated label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_class = ['top', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'trouser')"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Explore the data, display some input images\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "ids = np.random.randint(0,60000)\n",
    "plt.imshow(X_train[ids])\n",
    "plt.title(label_class[y_train[ids]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is time for the data preparation: data rescaling, label preparation.\n",
    "\n",
    "Hint: you can use the Keras function `to_categorical`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2])\n",
    "X_test= X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make the data preparation\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "import keras\n",
    "\n",
    "#scaler = StandardScaler()\n",
    "#X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "X_train = X_train / 255.\n",
    "\n",
    "y_train_vector = keras.utils.to_categorical(y_train, num_classes = 10)\n",
    "y_test_vector = keras.utils.to_categorical(y_test, num_classes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([9, 0, 0, ..., 3, 0, 5], dtype=uint8),\n",
       " array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32))"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train, y_train_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000,), (60000, 10))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_train_vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step: model building with Keras. Build your neural network architecture. At first, I would recommend a light architecture: 2 hidden layers, with 10 units per layer. Put that model into a function, so that you can reuse it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "# TODO: Build your model\n",
    "def model_two_layers(input_dim):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add the first Dense layers of 100 units with the input dimension\n",
    "    model.add(Dense(100, input_dim=input_dim, activation='relu'))\n",
    "\n",
    "    # Add four more layers of 100 units\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    \n",
    "    # Add finally the output layer with one unit: the predicted result\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.5351 - acc: 0.8105 - val_loss: 2.4856 - val_acc: 0.8422\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 8s 129us/step - loss: 0.3803 - acc: 0.8610 - val_loss: 2.8904 - val_acc: 0.8179\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 8s 130us/step - loss: 0.3430 - acc: 0.8738 - val_loss: 2.3370 - val_acc: 0.8524\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 8s 130us/step - loss: 0.3162 - acc: 0.8828 - val_loss: 2.4636 - val_acc: 0.8431\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 8s 131us/step - loss: 0.3005 - acc: 0.8869 - val_loss: 2.8705 - val_acc: 0.8185\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 8s 132us/step - loss: 0.2864 - acc: 0.8935 - val_loss: 2.2356 - val_acc: 0.8588\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 8s 133us/step - loss: 0.2745 - acc: 0.8971 - val_loss: 2.7966 - val_acc: 0.8226\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 8s 132us/step - loss: 0.2616 - acc: 0.9019 - val_loss: 2.7241 - val_acc: 0.8274\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 8s 137us/step - loss: 0.2537 - acc: 0.9046 - val_loss: 2.0384 - val_acc: 0.8700\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 8s 136us/step - loss: 0.2465 - acc: 0.9065 - val_loss: 2.4261 - val_acc: 0.8463\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 8s 135us/step - loss: 0.2359 - acc: 0.9107 - val_loss: 2.2390 - val_acc: 0.8576\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 8s 138us/step - loss: 0.2309 - acc: 0.9118 - val_loss: 2.8795 - val_acc: 0.8179\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 8s 135us/step - loss: 0.2244 - acc: 0.9140 - val_loss: 2.4553 - val_acc: 0.8441\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 8s 136us/step - loss: 0.2153 - acc: 0.9177 - val_loss: 2.5144 - val_acc: 0.8402\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 8s 137us/step - loss: 0.2092 - acc: 0.9208 - val_loss: 2.2769 - val_acc: 0.8561\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 8s 137us/step - loss: 0.2034 - acc: 0.9224 - val_loss: 2.1100 - val_acc: 0.8664\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 8s 138us/step - loss: 0.1967 - acc: 0.9247 - val_loss: 2.2601 - val_acc: 0.8568\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 8s 130us/step - loss: 0.1942 - acc: 0.9254 - val_loss: 2.1977 - val_acc: 0.8598\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 0.1896 - acc: 0.9269 - val_loss: 2.2111 - val_acc: 0.8598\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 10s 166us/step - loss: 0.1814 - acc: 0.9283 - val_loss: 2.2252 - val_acc: 0.8597\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 8s 133us/step - loss: 0.1830 - acc: 0.9287 - val_loss: 2.4363 - val_acc: 0.8459\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 8s 140us/step - loss: 0.1734 - acc: 0.9336 - val_loss: 2.0885 - val_acc: 0.8677\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 8s 138us/step - loss: 0.1707 - acc: 0.9330 - val_loss: 2.5204 - val_acc: 0.8399\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 8s 132us/step - loss: 0.1662 - acc: 0.9348 - val_loss: 2.4098 - val_acc: 0.8473\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.1655 - acc: 0.9358 - val_loss: 2.3178 - val_acc: 0.8532\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 9s 154us/step - loss: 0.1580 - acc: 0.9385 - val_loss: 2.3640 - val_acc: 0.8505\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 8s 136us/step - loss: 0.1553 - acc: 0.9399 - val_loss: 2.4210 - val_acc: 0.8476\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 8s 132us/step - loss: 0.1541 - acc: 0.9401 - val_loss: 2.1101 - val_acc: 0.8671\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 8s 132us/step - loss: 0.1489 - acc: 0.9419 - val_loss: 2.4002 - val_acc: 0.8489\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 8s 132us/step - loss: 0.1420 - acc: 0.9443 - val_loss: 2.0928 - val_acc: 0.8680\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 8s 132us/step - loss: 0.1446 - acc: 0.9430 - val_loss: 2.2703 - val_acc: 0.8571\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.1410 - acc: 0.9437 - val_loss: 2.4026 - val_acc: 0.8480\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 9s 154us/step - loss: 0.1375 - acc: 0.9468 - val_loss: 2.2978 - val_acc: 0.8549\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 8s 134us/step - loss: 0.1359 - acc: 0.9465 - val_loss: 2.4723 - val_acc: 0.8445\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 7s 125us/step - loss: 0.1333 - acc: 0.9478 - val_loss: 2.2110 - val_acc: 0.8610\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.1309 - acc: 0.9491 - val_loss: 2.5039 - val_acc: 0.8425\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 8s 131us/step - loss: 0.1311 - acc: 0.9502 - val_loss: 2.2409 - val_acc: 0.8580\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 8s 135us/step - loss: 0.1258 - acc: 0.9514 - val_loss: 2.3421 - val_acc: 0.8521\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 8s 141us/step - loss: 0.1250 - acc: 0.9513 - val_loss: 2.6339 - val_acc: 0.8353\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 8s 136us/step - loss: 0.1231 - acc: 0.9529 - val_loss: 2.4763 - val_acc: 0.8437\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 8s 138us/step - loss: 0.1200 - acc: 0.9527 - val_loss: 2.4216 - val_acc: 0.8484\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 8s 135us/step - loss: 0.1171 - acc: 0.9541 - val_loss: 2.4608 - val_acc: 0.8450\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 8s 140us/step - loss: 0.1154 - acc: 0.9548 - val_loss: 2.0961 - val_acc: 0.8677\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 8s 131us/step - loss: 0.1170 - acc: 0.9543 - val_loss: 2.3060 - val_acc: 0.8553\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 8s 129us/step - loss: 0.1095 - acc: 0.9567 - val_loss: 2.2709 - val_acc: 0.8565\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 8s 129us/step - loss: 0.1109 - acc: 0.9568 - val_loss: 2.3133 - val_acc: 0.8546\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 8s 130us/step - loss: 0.1045 - acc: 0.9591 - val_loss: 2.2973 - val_acc: 0.8554\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 8s 130us/step - loss: 0.1043 - acc: 0.9597 - val_loss: 2.2808 - val_acc: 0.8565\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 8s 129us/step - loss: 0.1069 - acc: 0.9588 - val_loss: 2.4848 - val_acc: 0.8437\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 8s 131us/step - loss: 0.1082 - acc: 0.9589 - val_loss: 2.3641 - val_acc: 0.8507\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 8s 130us/step - loss: 0.1033 - acc: 0.9602 - val_loss: 2.2557 - val_acc: 0.8582\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 8s 131us/step - loss: 0.1024 - acc: 0.9599 - val_loss: 2.2884 - val_acc: 0.8564\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 8s 131us/step - loss: 0.1032 - acc: 0.9597 - val_loss: 2.2871 - val_acc: 0.8560\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 8s 130us/step - loss: 0.0955 - acc: 0.9623 - val_loss: 2.3973 - val_acc: 0.8486\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 8s 130us/step - loss: 0.0919 - acc: 0.9640 - val_loss: 2.3396 - val_acc: 0.8532\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 8s 136us/step - loss: 0.0987 - acc: 0.9610 - val_loss: 2.5610 - val_acc: 0.8398\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 8s 130us/step - loss: 0.0970 - acc: 0.9626 - val_loss: 2.2409 - val_acc: 0.8592\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 8s 130us/step - loss: 0.0890 - acc: 0.9656 - val_loss: 2.3279 - val_acc: 0.8539\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 7s 125us/step - loss: 0.0866 - acc: 0.9662 - val_loss: 2.3224 - val_acc: 0.8543\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 7s 123us/step - loss: 0.0925 - acc: 0.9641 - val_loss: 2.2010 - val_acc: 0.8618\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.0937 - acc: 0.9634 - val_loss: 2.4898 - val_acc: 0.8439\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 7s 123us/step - loss: 0.0857 - acc: 0.9669 - val_loss: 2.3287 - val_acc: 0.8537\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.0830 - acc: 0.9674 - val_loss: 2.5755 - val_acc: 0.8385\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 7s 118us/step - loss: 0.0856 - acc: 0.9669 - val_loss: 2.3385 - val_acc: 0.8535\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 7s 122us/step - loss: 0.0886 - acc: 0.9661 - val_loss: 2.1661 - val_acc: 0.8642\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.0788 - acc: 0.9691 - val_loss: 2.1400 - val_acc: 0.8664\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 7s 123us/step - loss: 0.0832 - acc: 0.9680 - val_loss: 2.4676 - val_acc: 0.8455\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 7s 123us/step - loss: 0.0805 - acc: 0.9690 - val_loss: 2.4935 - val_acc: 0.8435\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.0781 - acc: 0.9690 - val_loss: 2.3186 - val_acc: 0.8550\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 7s 125us/step - loss: 0.0816 - acc: 0.9686 - val_loss: 2.2413 - val_acc: 0.8593\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 7s 123us/step - loss: 0.0763 - acc: 0.9712 - val_loss: 2.5570 - val_acc: 0.8403\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 7s 123us/step - loss: 0.0755 - acc: 0.9708 - val_loss: 2.4633 - val_acc: 0.8454\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 7s 125us/step - loss: 0.0816 - acc: 0.9688 - val_loss: 2.3247 - val_acc: 0.8542\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 7s 125us/step - loss: 0.0757 - acc: 0.9706 - val_loss: 2.3588 - val_acc: 0.8515\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.0757 - acc: 0.9716 - val_loss: 2.3399 - val_acc: 0.8540\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 7s 123us/step - loss: 0.0734 - acc: 0.9717 - val_loss: 2.3187 - val_acc: 0.8550\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 7s 125us/step - loss: 0.0766 - acc: 0.9712 - val_loss: 2.1926 - val_acc: 0.8619\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 7s 125us/step - loss: 0.0757 - acc: 0.9717 - val_loss: 2.4812 - val_acc: 0.8446\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.0668 - acc: 0.9739 - val_loss: 2.4172 - val_acc: 0.8488\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.0713 - acc: 0.9728 - val_loss: 2.3484 - val_acc: 0.8528\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 8s 126us/step - loss: 0.0696 - acc: 0.9735 - val_loss: 2.2523 - val_acc: 0.8595\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.0727 - acc: 0.9723 - val_loss: 2.2060 - val_acc: 0.8617\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.0701 - acc: 0.9729 - val_loss: 2.5355 - val_acc: 0.8413\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 7s 125us/step - loss: 0.0669 - acc: 0.9744 - val_loss: 2.2508 - val_acc: 0.8594\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.0636 - acc: 0.9756 - val_loss: 2.2357 - val_acc: 0.8604\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 7s 125us/step - loss: 0.0648 - acc: 0.9752 - val_loss: 2.2269 - val_acc: 0.8610\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.0674 - acc: 0.9743 - val_loss: 2.6518 - val_acc: 0.8341\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.0679 - acc: 0.9743 - val_loss: 2.2333 - val_acc: 0.8600\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.0632 - acc: 0.9763 - val_loss: 2.4303 - val_acc: 0.8483\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 8s 125us/step - loss: 0.0668 - acc: 0.9748 - val_loss: 2.2277 - val_acc: 0.8606\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 7s 125us/step - loss: 0.0653 - acc: 0.9757 - val_loss: 2.5232 - val_acc: 0.8418\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 8s 126us/step - loss: 0.0612 - acc: 0.9770 - val_loss: 2.4626 - val_acc: 0.8462\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 7s 125us/step - loss: 0.0632 - acc: 0.9762 - val_loss: 2.3542 - val_acc: 0.8533\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.0627 - acc: 0.9772 - val_loss: 2.2283 - val_acc: 0.8602\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 7s 125us/step - loss: 0.0633 - acc: 0.9759 - val_loss: 2.4758 - val_acc: 0.8445\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 8s 127us/step - loss: 0.0555 - acc: 0.9793 - val_loss: 2.2526 - val_acc: 0.8594\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 7s 125us/step - loss: 0.0640 - acc: 0.9760 - val_loss: 2.5581 - val_acc: 0.8401\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 7s 125us/step - loss: 0.0564 - acc: 0.9785 - val_loss: 2.2557 - val_acc: 0.8585\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 8s 125us/step - loss: 0.0607 - acc: 0.9770 - val_loss: 2.3622 - val_acc: 0.8521\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 8s 125us/step - loss: 0.0640 - acc: 0.9773 - val_loss: 2.3524 - val_acc: 0.8532\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0c7536bfd0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "\n",
    "\n",
    "model = model_two_layers(input_dim=X_train.shape[1])\n",
    "\n",
    "# Compile the model with mean squared error (for regression)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "callbacks = [EarlyStopping(monitor='val_loss', min_delta=0, patience=5), TensorBoard(log_dir='./src', histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, update_freq='epoch') ]\n",
    "# Now fit the model on 500 epoches with a batch size of 64\n",
    "model.fit(X_train, y_train_vector, validation_data=(X_test, y_test_vector), epochs=100, batch_size=64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compile and fit your model on your training data. Since this is a multiclass classification, the loss is not `binary_crossentropy` anymore, but `categorical_crossentropy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Compile and fit your model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your model has been trained, compute the accuracy (and other metrics if you want) on the train and test dataset.\n",
    "\n",
    "Be careful, Keras returns softmax output (so an array of 10 values between 0 and 1, for which the sum is equal to 1). To compute correctly the accuracy, you have to convert that array into a categorical array with zeros and a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8532"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Compute the accuracy of your model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "score = model.evaluate(X_test, y_test_vector, verbose=0) \n",
    "score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8532"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "y_pred = model.predict(X_test)\n",
    "acc = sum([np.argmax(y_test_vector[i])==np.argmax(y_pred[i]) for i in range(10000)])/10000\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think of those results? Can you improve it by changing the number of layers? Of units per layer? The number of epochs? The activation functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play with some hyperparameters if you want to improve your results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
