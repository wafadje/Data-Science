{"cells":[{"metadata":{"_kg_hide-output":true,"_uuid":"cd54bbe549e3b24b064e8e3b19dc8b94c6af36e7"},"cell_type":"markdown","source":"Few pointers I have is;\n\n1. Are they toxic with targetted words on race/region/religion?\n2. Do they contain obscene words ? Are these questions long or short?\n3. And, dividing them into clusters will help my model predict - what cluster of insincerity does an insincere question lies in .... etc\n\nSo, let's check the head frame now.."},{"metadata":{"trusted":true,"_uuid":"3688ebb72afdabe682778e7a416c1e813fc85a96"},"cell_type":"code","source":"import seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport string\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\ntqdm.pandas()\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB,MultinomialNB,ComplementNB,BernoulliNB","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2db8b65159172a645d2277b31cb14d8d319ba3ef"},"cell_type":"code","source":"glove = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\nparagram =  '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\nwiki_news = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f75bcc423344df4c1cf989122ecce652b5214c50"},"cell_type":"code","source":"df = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\nprint(\"Train shape : \",df.shape)\nprint(\"Test shape : \",test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32f1d20313a7ba494eeb63b4486f8cf3d7ad18a6"},"cell_type":"code","source":"df.info()\ntest.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9112ec235c0b74c9e592d15ebf79259c8f30140"},"cell_type":"code","source":"df.head(n=2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8574976d5160017aedcbfc2a3e6202b93399e0ff"},"cell_type":"markdown","source":"## Explorer Dataset"},{"metadata":{"trusted":false,"_uuid":"938ebca8a765812c4e03c341a2235eed72aece50"},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"585dabb0ef46fafc2703498421daf4be2194b35a"},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3e1c2c62ee2efa30941d7f82ee1effa15f88c3d"},"cell_type":"code","source":"df.isnull().sum()\ntest.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"145f10d396d8ac6f7b826f5296483a4f77ef0278"},"cell_type":"markdown","source":"## Data Preprocessing"},{"metadata":{"trusted":false,"_uuid":"b60097ea46cff44d411e730ac702de6f9acdc02a"},"cell_type":"code","source":"df.where(df['target']==1).count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b0a2d72c32e6de45a04c3e5b634f447dba6d64b0"},"cell_type":"code","source":"df.where(df['target']==0).count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ea3e4ba59c3250d7127ff7a5e62d76bc1bfc555"},"cell_type":"code","source":"sincere_questions = df[df['target'] == 0]\ninsincere_questions = df[df['target'] == 1]\ninsincere_questions.tail(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1cd889639360c4e81011239ad91a1ee3ae91992c"},"cell_type":"markdown","source":"A large part of the data is unbalanced, but how can we solve it?"},{"metadata":{"_uuid":"974884021bb5596c55a00779a52057b75381c79b"},"cell_type":"markdown","source":"## Exploreing Questions"},{"metadata":{"trusted":false,"_uuid":"7605e2aaef47776f56d58e6702bec575ca3bedc6"},"cell_type":"code","source":"question = df['question_text']\ni=0\nfor q in question[:5]:\n    i=i+1\n    print('sample '+str(i)+':' ,q)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b4d398d11f857e4f0613a880152f3ff158b9b69"},"cell_type":"markdown","source":"## Some Feature Engineering"},{"metadata":{"trusted":true,"_uuid":"a91f143a592776aabb4deaad0ac69514158ca1bc"},"cell_type":"code","source":"df[\"num_words\"] = df[\"question_text\"].apply(lambda x: len(str(x).split()))\n\nprint('maximum of num_words in train',df[\"num_words\"].max())\nprint('min of num_words in train',df[\"num_words\"].min())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"cb8350c0a7040717db6253a2382ec394a5c52885"},"cell_type":"code","source":"df[\"num_unique_words\"] = df[\"question_text\"].apply(lambda x: len(set(str(x).split())))\n#test[\"num_unique_words\"] = test[\"question_text\"].apply(lambda x: len(set(str(x).split())))\nprint('maximum of num_unique_words in train',df[\"num_unique_words\"].max())\nprint('mean of num_unique_words in train',df[\"num_unique_words\"].mean())\n#print(\"maximum of num_unique_words in test\",test[\"num_unique_words\"].max())\n#print('mean of num_unique_words in train',test[\"num_unique_words\"].mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"922533de1f9fe549dbf3bb57658c191f440f1140"},"cell_type":"markdown","source":"#### Number of stopwords in the text"},{"metadata":{"trusted":false,"_uuid":"e42aaa03f2bec2a680b15c692615da3aaa77f413"},"cell_type":"code","source":"df[\"num_stopwords\"] = df[\"question_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n#test[\"num_stopwords\"] = test[\"question_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\nprint('maximum of num_stopwords in train',df[\"num_stopwords\"].max())\n#print(\"maximum of num_stopwords in test\",test[\"num_stopwords\"].max())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f878ee6a2bb6eb6d067d134d921032565cb53ec"},"cell_type":"markdown","source":"#### Number of punctuations in the text"},{"metadata":{"trusted":false,"_uuid":"f3380da4df2ae3aeaca6447fb232a18a077a16b3"},"cell_type":"code","source":"df[\"num_punctuations\"] =df['question_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n#test[\"num_punctuations\"] =test['question_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\nprint('maximum of num_punctuations in train',df[\"num_punctuations\"].max())\n#print(\"maximum of num_punctuations in test\",test[\"num_punctuations\"].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9700cd0de40a464f9d2acd843f8dca2d051d5e8"},"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nimport re\nfrom nltk.stem import WordNetLemmatizer\n\nlemm_ = WordNetLemmatizer()\nst = PorterStemmer()\nstops = set(stopwords.words(\"english\"))\ndef cleanData(text, lowercase = True, remove_stops = True, stemming = False, lemma = True):\n\n    txt = str(text)\n    txt = re.sub(r'[^a-zA-Z. ]+|(?<=\\\\d)\\\\s*(?=\\\\d)|(?<=\\\\D)\\\\s*(?=\\\\d)|(?<=\\\\d)\\\\s*(?=\\\\D)',r'',txt)\n    txt = re.sub(r'\\n',r' ',txt)\n    \n    #converting to lower case\n    if lowercase:\n        txt = \" \".join([w.lower() for w in txt.split()])\n    \n    # removing stop words\n    if remove_stops:\n        txt = \" \".join([w for w in txt.split() if w not in stops])\n    \n    # stemming\n    if stemming:\n        txt = \" \".join([st.stem(w) for w in txt.split()])\n        \n    if lemma:\n        txt = \" \".join([lemm_.lemmatize(w) for w in txt.split()])\n\n    return txt\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6436302da8dbd9e2f6916e133bf4e67812846249"},"cell_type":"code","source":"df['clean_question_text'] = df['question_text'].map(lambda x: cleanData(x))\ntest['clean_question_text'] = test['question_text'].map(lambda x: cleanData(x))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef656ab8ea71a8390424261063283c0a521e2762"},"cell_type":"code","source":"test['clean_question_text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3fc1f3b25a4cf51f338f0528c5ebf50b8629c86c"},"cell_type":"code","source":"max_features = 50000  ##More than this would filter in noise also\ntfidf_vectorizer = TfidfVectorizer(ngram_range =(2,4) , max_df=0.90, min_df=5, max_features=max_features) ##4828 features found\n#tfidf_feature_names = tfidf_vectorizer.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b6c935bd18550482edb4f5f733a1473a24a96317"},"cell_type":"code","source":"X = tfidf_vectorizer.fit_transform(df['clean_question_text'])\nX_te = tfidf_vectorizer.transform(test['clean_question_text'])\ntfidf_feature_names = tfidf_vectorizer.get_feature_names()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc9429ab3a1af0eb3b7d8f33e6251108fa6ebae1"},"cell_type":"code","source":"from gensim.models import KeyedVectors\n\nnews_path = '../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\nembeddings_index = KeyedVectors.load_word2vec_format(news_path, binary=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2490cabdc73ac33f17dbc83b441414f2d37f1aac"},"cell_type":"code","source":"y = df[\"target\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56ac7d41ea9c180d3c0e383ae2f0632340e1a068"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1beccd77969801078b7411cab4f02deaf50a97d7"},"cell_type":"code","source":"# Classification and prediction\nclf = LogisticRegression(C=10, penalty='l1')\nclf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"518ed5d2a84052b45d8e22a9771f10fa4d4b3e0c"},"cell_type":"code","source":"clf.score(X_val, y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be6c718b85a94e53b78613c2d1cef914ed4d5a51"},"cell_type":"code","source":"p_test = clf.predict_proba(X_te)[:, 0]\ny_te = (p_test > 0.5).astype(np.int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5887430123a03811c1f99edc13db1a0348d3bc5"},"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nimport nltk\n\nstop = stopwords.words('english')\nstemmer = PorterStemmer()\nlem = nltk.WordNetLemmatizer()\neng_stopwords = set(stopwords.words(\"english\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92d487253716fa3211fd82d810e2a7ade7678147"},"cell_type":"code","source":"#df['question_text'] = df['question_text'].apply(lambda x: x.lower())\ndf['question'] = df['question_text'].str.replace(r\"[^a-z0-9 ]\", '')\ndf['tokens'] = df['question'].apply(word_tokenize)\ndf['tokens'] = df['tokens'].map(lambda x: [word for word in x if word not in eng_stopwords])\ndf['lems'] = df['tokens'].map(lambda x: [lem.lemmatize(word) for word in x])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b990fc2766ed2d5c353d2ef22f598b24aa961a5f"},"cell_type":"code","source":"df['lems']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12b7e562f478d8601c3a4985f3c7da8790face26"},"cell_type":"code","source":"from gensim.corpora import Dictionary\n\ndicti = Dictionary(df['lems'])\n\nbow = [dicti.doc2bow(line) for line in df['lems']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb3949843c30322d760add38ecc19ad4de5ee0aa"},"cell_type":"code","source":"# TODO: Compute TF-IDF\nfrom gensim.models import TfidfModel\n\ntfmodel = TfidfModel(bow)\n\ntfidf = tfmodel[bow]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd802be4adf99ce6e92b6a9229117338c113ac45"},"cell_type":"code","source":"from gensim.models import LsiModel\n\nlsa = LsiModel(corpus = tfidf, num_topics=10, id2word = dicti)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3df3c6442e3c1e6e583f07933242c322ba42032b"},"cell_type":"code","source":"from pprint import pprint\n\npprint(lsa.print_topics(num_words=4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2fdc4ca1047721cf6d0313ab5da5718c9268c787"},"cell_type":"code","source":"from gensim.models import LdaModel\nlda = LdaModel(corpus = tfidf, num_topics=6, id2word = dicti, passes=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b91efc4116f57acf67a661143e31fd19734f4e22"},"cell_type":"code","source":"pprint(lda.print_topics(num_words=4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5676c644ea2a35d673d152f5b83fb8ab8e87cf12"},"cell_type":"code","source":"import pyLDAvis\n\nimport pyLDAvis.gensim\n\n# Visualize the topics\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda, bow, dicti)\nvis","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d8bec0d4a50fa5a29a20c158b7c0a333626111f"},"cell_type":"markdown","source":"# Data Visualization"},{"metadata":{"trusted":true,"_uuid":"4e679ccce017e13418f5f245ce271f480274ce52"},"cell_type":"code","source":"ax=sns.countplot(x='target',hue=\"target\", data=df  ,linewidth=1,edgecolor=sns.color_palette(\"dark\", 3))\nplt.title('Data set distribution');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd4b5b9ab19672efb162fc8900b1581f247c726e"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split \nimport nltk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc653f93c016ed735222ab8fce69553b07778e20"},"cell_type":"code","source":"def build_vocab(sentences, verbose =  True):\n    \"\"\"\n    :param sentences: list of list of words\n    :return: dictionary of words and their count\n    \"\"\"\n    vocab = {}\n    for sentence in tqdm(sentences, disable = (not verbose)):\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87cc2d1006594518516cbecbafe56cafe2f6b7c0"},"cell_type":"code","source":"def clean_text(x):\n\n    x = str(x)\n    for punct in \"/-'\":\n        x = x.replace(punct, ' ')\n    for punct in '&':\n        x = x.replace(punct, f' {punct} ')\n    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n        x = x.replace(punct, '')\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"176b0afd0419deb03c3b39d07da6f0e22149621b"},"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nimport nltk\n\nstop = stopwords.words('english')\nstemmer = PorterStemmer()\nlem = nltk.WordNetLemmatizer()\neng_stopwords = set(stopwords.words(\"english\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1a317146a6cd3d955030e8edabf94b2d3a6b56e"},"cell_type":"code","source":"df[\"question_text\"] = df[\"question_text\"].apply(lambda x: clean_numbers(x))\ndf[\"question_text\"] = df[\"question_text\"].apply(lambda x: clean_text(x))\ndf['lowered_question'] = df['question_text'].apply(lambda x: x.lower())\ndf['question'] = df['lowered_question'].str.replace(r\"[^a-z0-9 ]\", '')\ndf['tokens'] = df['question'].apply(nltk.word_tokenize)\ndf['tokens'] = df['tokens'].apply(lambda x: [word for word in x if word not in eng_stopwords])\ndf['lems'] = df['tokens'].apply(lambda x: [lem.lemmatize(word) for word in x])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f366a5053dec622034a3f9825d247e80017b1bdf"},"cell_type":"code","source":"df_train = pd.DataFrame(df['lems'])\ndf_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c21ef472d17aa64dad9c694c1d3e9e543f7cadc"},"cell_type":"code","source":"def clean_numbers(x):\n\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be186d3204c6b71ffdd06f0dcdf477a3541bcce6"},"cell_type":"code","source":"import re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7ecce418562ccfc6fcbd6e27d8fe2f8cad15fd2"},"cell_type":"code","source":"sentences = df_train['lems'].apply(lambda x: x.split())\nvocab = build_vocab(sentences)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21cd434fbf53e795ee2f53c784d7b070f382d0ba"},"cell_type":"code","source":"import operator \n\ndef check_coverage(vocab,embeddings_index):\n    a = {}\n    oov = {}\n    k = 0\n    i = 0\n    for word in tqdm(vocab):\n        try:\n            a[word] = embeddings_index[word]\n            k += vocab[word]\n        except:\n\n            oov[word] = vocab[word]\n            i += vocab[word]\n            pass\n\n    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n\n    return sorted_x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a149f86a98062d85d25e73284e2e01daf15eeaba"},"cell_type":"code","source":"oov = check_coverage(vocab,embeddings_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4969dc32b78a7c959ae38b7e17284af9620a60d"},"cell_type":"code","source":"oov[:10]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5c61cb8ce789177c6c799da27837b803173b3bd0"},"cell_type":"markdown","source":"\nBasic Logistic Regression"},{"metadata":{"trusted":true,"_uuid":"9909b36a651263ed0530c73e35da63bc1ff103eb"},"cell_type":"code","source":"train_text['lems'] = df['lems']\ntrain_text['lems']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1efb6c50c5cd53b306b3f2036be0df272871c1b7"},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3323696952cb96e21dc19bf6eee9726a2c348c97"},"cell_type":"code","source":"tfidf_vectorizer = TfidfVectorizer()\ntfidf_vectorizer.fit(all_text)\n\ncount_vectorizer = CountVectorizer()\ncount_vectorizer.fit(all_text)\n\ntrain_text_features_cv = count_vectorizer.transform(train_text)\ntest_text_features_cv = count_vectorizer.transform(test_text)\n\ntrain_text_features_tf = tfidf_vectorizer.transform(train_text)\ntest_text_features_tf = tfidf_vectorizer.transform(test_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7aec8e992349c54e75c9eda8ac35a5bd92f60375"},"cell_type":"code","source":"kfold = KFold(n_splits = 5, shuffle = True, random_state = 2018)\ntest_preds = 0\noof_preds = np.zeros([df.shape[0],])\n\nfor i, (train_idx,valid_idx) in enumerate(kfold.split(df['lems'])):\n    x_train, x_valid = train_text_features_tf[train_idx,:], train_text_features_tf[valid_idx,:]\n    y_train, y_valid = train_target[train_idx], train_target[valid_idx]\n    classifier = LogisticRegression()\n    print('fitting.......')\n    classifier.fit(x_train,y_train)\n    print('predicting......')\n    print('\\n')\n    oof_preds[valid_idx] = classifier.predict_proba(x_valid)[:,1]\n    test_preds += 0.2*classifier.predict_proba(test_text_features_tf)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"71241eeb79367d17082f2ec34c28fbd29d21d60b"},"cell_type":"code","source":"pred_train = (oof_preds > .25).astype(np.int)\nf1_score(train_target, pred_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7c0ad5db9a3c2aee1a719177052e92d20392f0e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}
